{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP3HQHKFAoz3HowwFZZsvtB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Alirezamirbagheri/Machine-Learning-Transfer-learning-for-configurations-control/blob/main/Transfer_Learning_for_Configuration_Control.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. Libraries**"
      ],
      "metadata": {
        "id": "xdOagxwxjsMT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "1jjxFafBgVSp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d2597a7-205b-4db7-9687-5fdb230071ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchmetrics in /usr/local/lib/python3.12/dist-packages (1.8.2)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (2.0.2)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (25.0)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (2.8.0+cu126)\n",
            "Requirement already satisfied: lightning-utilities>=0.8.0 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (0.15.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (75.2.0)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.12/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.15.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.20.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->torchmetrics) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->torchmetrics) (3.0.3)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import PIL.Image as Image\n",
        "import matplotlib.pylab as plt\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import pathlib\n",
        "from torchsummary import summary\n",
        "from tqdm import tqdm\n",
        "from torch.optim import Adam\n",
        "import torch.nn as nn\n",
        "from torchvision.transforms import transforms\n",
        "import os\n",
        "!pip install torchmetrics\n",
        "from torchmetrics import ConfusionMatrix"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Numpy version: \" + np.__version__)\n",
        "print(\"PIL.Image version: \" + Image.__version__)\n",
        "print(\"Matplotlib.pylab version: \" + plt.__version__)"
      ],
      "metadata": {
        "id": "2UfrjzxHhLRY",
        "outputId": "82995f1c-d36d-41cf-dead-9a20b27b2713",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Numpy version: 2.0.2\n",
            "PIL.Image version: 11.3.0\n",
            "Matplotlib.pylab version: 2.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. Data preparation and import**\n",
        "In this Notebook Google Drive is used."
      ],
      "metadata": {
        "id": "LpPvF8LFj7jx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "V7ILpldWhLdZ",
        "outputId": "57b4a4cf-49d2-40d6-9f3f-ac66fb999fe4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 **Loading the test data**\n",
        "The data set is already split into a **training**, **validation** and  **test** sets.  The class names are derived from the sub folder names."
      ],
      "metadata": {
        "id": "33D2vr10l955"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For a CNN, the images must have the same dimensions (height × width × channels).\n",
        "Because:\n",
        "- Convolution layers slide fixed-size filters over the image.\n",
        "- If images had different sizes, the output feature maps would also have different shapes → impossible to batch them together.\n",
        "- Dense (fully connected) layers at the end require a fixed input size.\n",
        "\n",
        "Here we define a variable called **IMAGE_SHAPE**."
      ],
      "metadata": {
        "id": "Pw6cJ0N_mc1D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "IMAGE_SHAPE = (224, 224)\n",
        "image_generator = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1/255)\n",
        "training_data=\"./drive/MyDrive/ML/Transfer_Learning/dataset_truck/training\"\n",
        "training_image_data  = image_generator.flow_from_directory(training_data,target_size=IMAGE_SHAPE)\n",
        "\n",
        "# ImageDataGenerator, a utility for preprocessing and augmenting image data before feeding it into a neural network.\n",
        "# Automatically load images from disk. Preprocess them (e.g., rescale, rotate, flip, shift, etc.). Feed batches of preprocessed images into your model during training.\n",
        "# This makes a generator but the data not loaded yet\n",
        "# This normalization helps neural networks train more efficiently and stably.\n",
        "# rescale=1./255 → normalizes pixel values from [0,255] → [0,1].\n",
        "# target_size=(224,224) → resizes every image to the input size required by your CNN (e.g., ResNet, MobileNet, VGG all expect 224×224).\n",
        "# flow_from_directory → automatically assigns labels from folder names inside training_data.\n",
        "# Feed batches of preprocessed images into your model during training."
      ],
      "metadata": {
        "id": "StyhK07mhLtd",
        "outputId": "25ec96c5-df9b-4d24-ce49-ff08844f87b0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 60 images belonging to 6 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "BATCH = 32\n",
        "EPOCH = 5"
      ],
      "metadata": {
        "id": "AcaMW8pnhLwi"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 Loading the validation data"
      ],
      "metadata": {
        "id": "oSFJK7L1ClC8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_generator = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1/255)\n",
        "validation_data=\"./drive/MyDrive/ML/Transfer_Learning/dataset_truck/validation\"\n",
        "validation_data_image_data  = image_generator.flow_from_directory(validation_data,target_size=IMAGE_SHAPE)"
      ],
      "metadata": {
        "id": "q7TN0QighMDE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ca93613-4ef5-46ce-8222-9d65cbf47e22"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 150 images belonging to 6 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. The pre-trained base model**\n",
        "A pre-trained model is used for Transfer Learning.\n",
        "For the use of this model there are two possibilities:\n",
        "\n",
        "\n",
        "* Feature Extractor: Use of the learned features for your own application.\n",
        "* Fine Tuning: Re-training of the Base Model for your own application.\n",
        "\n",
        "In the following, we will use the base model as a feature extractor and create our own classification layer for our application.\n",
        "\n",
        "## 4.1 Structure and use of the basic model - MobilNetV2\n",
        "Various pre-trained network architectures are already available for download. Often these are pre-trained with the [ImageNet dataset](http://www.image-net.org/).\n",
        "\n",
        "Here we use the MobileNetV2 model as a pre-trained base model. The torchsummary.summary() command can be used to output the architecture of the model to get an overview. It is necessary to give a network and a shape of images as inputs."
      ],
      "metadata": {
        "id": "oBfdcMHxAceo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we want to use a model as a feature extractor, we need to fit it to our own classification task. The classification layer of the pre-trained model must thus be replaced by one of our own.\n",
        "\n",
        "To simplify this, we load a version of MobilNetV2, which is already prepared for use as a feature extractor - the classification layer is already removed."
      ],
      "metadata": {
        "id": "g_NcBFokDBxA"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DQm0fzEaCqk_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}